# SAE-LoRA: Interpretable Finetuning via SAE Latent Space

Train low-rank adapters that read and write SAE feature activations instead of dense residual stream vectors. The adapter's learned weights directly reference interpretable SAE latents, so with sparsity pressure the finetuned behavior should be describable as simple relationships between known features.

## Method

Given frozen model weights and a frozen SAE at layer L:

```
latents = SAE_encode(residual)
residual += SAE_decode(norm(A) @ latents @ norm(B) * s)
```

A (d_sae x r) and B (r x d_sae) are the trainable low-rank matrices, s (r x 1) is a learned scale. A and B are row/column-normalized so only their directions are learned. Loss is per-token cross-entropy on assistant responses plus L1 on A and B weights.

Training data is generated by LLM-classifying prompts from HelpSteer by topic, then LLM-rewriting matching responses to exhibit a target behavior. The model sees a balanced mix of modified and unmodified examples.

## Setup

Gemma-2-9b-it with gemma-scope residual SAE at layer 31 (131K latents, top-k). Rank 1, ~500 balanced examples, lr=1e-4, l1_weight=0.05.

## Results

A rank-1 adapter successfully learns "if math prompt, respond in french" and generalizes to held-out prompts. However, the learned weights are not sparse despite L1 penalty â€” they look only marginally more structured than noise, and top-weight features aren't clearly interpretable. The core open question is whether this is fixable (stronger sparsity, different parameterization, rank) or a fundamental limitation.
